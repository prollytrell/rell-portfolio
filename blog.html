<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Blog | Governance, Risk, and Compliance with AI – Terrell Grenyion</title>

  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@300;400;500;600&family=JetBrains+Mono:wght@500;600;700&display=swap" rel="stylesheet">

    <script>
    (function () {
      const STORAGE_KEY = 'siteTheme';
      const root = document.documentElement;

      function apply(theme) {
        if (theme === 'light') {
          root.classList.add('theme-light');
        } else {
          root.classList.remove('theme-light');
        }
      }

      let stored = null;
      try {
        stored = sessionStorage.getItem(STORAGE_KEY);
      } catch (e) {
        stored = null;
      }

      if (stored === 'light' || stored === 'dark') {
        apply(stored);
      } else {
        // default to dark
        apply('dark');
      }

      window.__setTheme = function (theme) {
        apply(theme);
        try {
          sessionStorage.setItem(STORAGE_KEY, theme);
        } catch (e) {}
      };

      window.__toggleTheme = function () {
        const isLight = !root.classList.contains('theme-light');
        window.__setTheme(isLight ? 'light' : 'dark');
      };
    })();
  </script>


  <link rel="stylesheet" href="styles.css" />
</head>
<body>
  <header class="site-header">
  <div class="container header-inner">
    <div class="logo-mark">
      <span class="logo-symbol">&gt;</span>
      <span class="logo-text">Terrell<span class="accent">.GRENYION</span></span>
    </div>

    <div class="header-right">
      <nav class="main-nav">
        <!-- keep your existing links, with the correct .active on each page -->
        <a href="index.html" class="nav-link">Home</a>
        <a href="projects.html" class="nav-link">Projects</a>
        <a href="blog.html" class="nav-link">Blog</a>
      </nav>

      <!-- Theme toggle button -->
      <button class="theme-toggle" type="button" aria-label="Toggle color theme">
        <span class="theme-toggle-track">
          <span class="theme-toggle-thumb"></span>
        </span>
        <span class="theme-toggle-label">
          <span class="theme-toggle-label-light">LIGHT</span>
          <span class="theme-toggle-label-dark">DARK</span>
        </span>
      </button>
    </div>
  </div>
</header>


  <main>
  <section class="section">
    <div class="container blog-layout">
      <div class="blog-list">

        <!-- Blog Article 3 -->
  <article class="blog-card">
    <header class="blog-card-header">
      <p class="hero-kicker">BLOG • Security Awareness</p>
      <h1 class="hero-title">Strengthening Your Account Security With Practical, Everyday Habits</h1>
      <p class="hero-subtitle">Why password reuse, weak MFA habits, and outdated login methods put you at risk and what actually works instead</p>

      <div class="blog-meta">
        <span>By Terrell Grenyion</span>
        <span class="divider">•</span>
        <span>2025</span>
        <span class="divider">•</span>
        <span>~3–5 min read</span>
      </div>

      <div class="blog-toggle-row">
        <button class="btn ghost-btn blog-toggle-btn" data-expand-target="article-1-wrapper">
          Read more
        </button>
      </div>
    </header>

    <div class="blog-expand-wrapper" id="article-1-wrapper" aria-expanded="false">
      <article class="blog-article">
        <section class="article-section">
  <h2>Why Password Reuse Is One of the Biggest Risks</h2>
  <p>I used to reuse the same password everywhere because it felt easier. Most people do. The problem is that it only takes one breach for an attacker to walk straight into your other accounts. Attackers use automated credential stuffing tools to test leaked passwords across hundreds of sites. If you reuse passwords, they eventually find a match.</p>
</section>

<section class="article-section">
  <h2>A Better Approach to Managing Your Logins</h2>
  <p>No one can memorize dozens of strong, unique passwords. That’s why relying on memory never works long term. Password vaults solve this problem by generating and storing unique passwords for every site in an encrypted way. They can be compromised like any other tool, but the security benefit is still far greater than relying on weak or reused passwords. Avoid storing passwords in your browser since those aren’t protected the same way.</p>
</section>

<section class="article-section">
  <h2>Use MFA Everywhere and Stay Aware of MFA Fatigue</h2>
  <p>MFA is still one of the strongest defenses you can enable. App based MFA and hardware keys are far stronger than SMS codes. Be mindful of MFA fatigue attacks where an attacker sends repeated approval prompts hoping you accept one by accident. Treat your security questions like passwords instead of using predictable personal information.</p>
</section>

<section class="article-section">
  <h2>Passkeys Are the Future of Authentication</h2>
  <p>More platforms are moving toward passkeys which remove passwords completely. They’re phishing resistant and can’t be reused or stolen the way traditional passwords can. Whenever a site offers passkeys, that’s the best option.</p>
</section>

<section class="article-section">
  <h2>Protect Your Devices Too</h2>
  <p>Even strong login practices won’t help if your device itself is compromised. Make sure you keep your OS and apps updated, avoid untrusted browser extensions, and lock down your device settings. Your account security starts with a secure device.</p>
</section>

<section class="article-section">
  <h2>Final Thoughts</h2>
  <p>You don’t need to be perfect to improve your security. A few consistent habits make you much harder to attack. Use unique passwords, enable MFA, adopt passkeys when possible, and keep your devices secure. These small steps close the door on the most common attack paths.</p>
</section>
      </article>
    </div>
  </article>
  <!-- Blog Article 3 End -->
        <!-- Blog Article 2 (most recent) -->
        <article class="blog-card">
          <header class="blog-card-header">
            <p class="hero-kicker">BLOG • INVESTIGATION</p>
            <h1 class="hero-title">Breaking Down a Strange reCAPTCHA Event I Looked Into Recently</h1>
            <p class="hero-subtitle">
              A closer look at how a normal-looking reCAPTCHA event was tied to obfuscated scripts, silent PowerShell
              activity, and persistence techniques that only really made sense once the full timeline was laid out.
            </p>

            <div class="blog-meta">
              <span>By Terrell Grenyion</span>
              <span class="divider">•</span>
              <span>2025</span>
              <span class="divider">•</span>
              <span>~4–6 min read</span>
            </div>

            <div class="blog-toggle-row">
              <button class="btn ghost-btn blog-toggle-btn" data-expand-target="article-2-wrapper">
                Read more
              </button>
            </div>
          </header>

          <div class="blog-expand-wrapper" id="article-2-wrapper" aria-expanded="false">
            <article class="blog-article">
              <!-- Blog Article 2 -->
              <section class="article-section">
                <h2>Breaking Down a Strange reCAPTCHA Event I Looked Into Recently</h2>
                <p>
                  I ran into an interesting situation involving a real Google reCAPTCHA that turned into something much
                  bigger once I started going through the activity around it. A user landed on a page they believed was
                  part of their normal workflow, and the site even presented a real reCAPTCHA. Nothing stood out at
                  first. The iframe loaded the same way it would on any other site.
                </p>
                <p>
                  The part that made me stop and look again was everything tied to what happened after that interaction.
                  The timing lined up with behavior that didn’t feel normal, and the deeper I went, the more the pattern
                  started to make sense.
                </p>
              </section>

              <section class="article-section">
                <h2>What the Logs Showed</h2>
                <p>
                  The first thing that stood out was an obfuscated script reaching out to a malicious site. It was
                  written in a way that tried to hide what it was doing, but once you start breaking it apart, the intent
                  becomes clear. After that initial step, I noticed callbacks to suspicious IPs and URLs and references
                  to PHP commands. Some of the traffic pointed back to klick-style URLs, which helped line up the moment
                  the user interacted with the page.
                </p>
                <p>
                  While I worked through the telemetry, the system activity started to form a common attack pattern.
                  There were silent and headless PowerShell executions. Files were being downloaded, run, then deleted
                  right after. That type of create–execute–delete behavior is something you see often when an attacker
                  wants the action without the evidence left behind. There were also scheduled tasks created for
                  persistence, which tied everything together.
                </p>
                <p>
                  None of these things on their own would immediately scream reCAPTCHA trouble. The reCAPTCHA was real.
                  The page looked real. The activity was quiet. But putting them together removed the guesswork.
                </p>
              </section>

              <section class="article-section">
                <h2>Why the Timing Mattered So Much</h2>
                <p>
                  One thing that stood out was how fast everything moved once the user interacted with the page. No
                  credentials were typed. The browser already had a valid session, and the attacker used that
                  opportunity. When you lay out the sequence by timestamp, it becomes much easier to see how normal user
                  behavior can blend into attacker behavior in a matter of seconds.
                </p>
                <p>
                  That is what made the reCAPTCHA moment important. It wasn’t the problem. It was the timing window the
                  attacker used.
                </p>
              </section>

              <section class="article-section">
                <h2>The Indicators That Pulled It All Together</h2>
                <p>
                  The main things that helped confirm what was happening were consistent. On their own, each one was
                  suspicious. Together, they told a clear story.
                </p>
                <ul class="project-bullets">
                  <li>Obfuscated script activity with outbound calls</li>
                  <li>Silent or headless PowerShell execution</li>
                  <li>Sudden scheduled tasks that no user created</li>
                  <li>Malicious or suspicious URLs in the callback chain</li>
                  <li>Files that appear briefly, execute, and then disappear</li>
                </ul>
                <p>
                  Each of these pieces pointed in the same direction, and the timeline supported it. Once the activity
                  was laid out step by step, the attack path stopped being a guess and became something you could clearly
                  explain.
                </p>
              </section>

              <section class="article-section">
                <h2>How to Respond</h2>
                <p>
                  Situations like this always call for a full cleanup. You want to revoke sessions, reset credentials and
                  MFA, isolate the device, block the malicious URLs and IPs, and check for anything that might allow the
                  attacker back in. That includes scheduled tasks, browser extensions, stored sessions, and anything else
                  that could have been manipulated.
                </p>
                <p>
                  The user side matters too. Even when something looks familiar, like a real reCAPTCHA on a normal page,
                  there’s value in slowing down, checking URLs, and avoiding links that fall outside of expected
                  workflows. Conditional access, EDR controls, and tighter rules around script-based execution go a long
                  way here when an attacker is trying to quietly abuse an existing session instead of stealing credentials
                  outright.
                </p>
              </section>

              <section class="article-section">
                <h2>Skills I Sharpened While Working Through It</h2>
                <p>
                  I had to pull from different areas to break everything down. Threat analysis and narrowing my queries
                  helped focus on the right parts of the telemetry instead of getting lost in the noise. Deobfuscating
                  scripts and reviewing persistence behavior played a big role in understanding what the attacker wanted
                  to accomplish and how they tried to hide it.
                </p>
                <p>
                  Understanding session and token behavior helped confirm the hijacking attempts and why the attacker
                  didn’t need the user to type anything. I also spent time verifying indicators through OSINT and
                  checking domain and IP reputations.
                </p>
                <p>
                  One thing I appreciated was being able to use locally run AI tools to analyze some of the obfuscated
                  commands. It kept sensitive data off third-party services and saved time.
                </p>
              </section>

              <section class="article-section">
                <h2>Final Thoughts</h2>
                <p>
                  This whole situation reminded me that attackers hide in normal-looking activity more often than people
                  realize. Everything seems fine until you line up the user action with what the system did next. That’s
                  when the picture becomes clear.
                </p>
                <p>
                  It also showed how fast a session can be misused without the user typing anything. Even controls we
                  trust, like reCAPTCHA, can fit into an attack chain if someone knows how to use the timing to their
                  advantage. This was a solid learning experience and a good reminder to always look at behavior patterns
                  instead of isolated events.
                </p>
              </section>
              <!-- Blog Article 2 End -->
            </article>
          </div>
        </article>

        <!-- Blog Article 1 (AI GRC white paper) -->
        <article class="blog-card">
          <header class="blog-card-header">
            <p class="hero-kicker">BLOG • RESEARCH</p>
            <h1 class="hero-title">Governance, Risk, and Compliance with Artificial Intelligence</h1>
            <p class="hero-subtitle">
              Building a framework for responsible enterprise AI: aligning governance, risk management, and compliance
              with emerging AI risks, regulations, and ethical expectations.
            </p>

            <div class="blog-meta">
              <span>By Terrell Grenyion</span>
              <span class="divider">•</span>
              <span>2025</span>
              <span class="divider">•</span>
              <span>~10–12 min read</span>
            </div>

            <div class="blog-toggle-row">
              <button class="btn ghost-btn blog-toggle-btn" data-expand-target="article-1-wrapper">
                Read more
              </button>
            </div>
          </header>

          <div class="blog-expand-wrapper" id="article-1-wrapper" aria-expanded="false">
            <article class="blog-article">
              <section class="article-section">
            <h2>Abstract</h2>
            <p>
              As artificial intelligence (AI) becomes more integrated into business operations, traditional governance,
              risk management, and compliance (GRC) models face challenges in addressing its unique ethical, legal, and
              technical issues. This paper examines how organizations can adapt their governance strategies to ensure
              the responsible use of AI. It analyzes key frameworks, including the NIST AI Risk Management Framework and
              the EU AI Act, and identifies gaps in oversight, transparency, and accountability.
            </p>
            <p>
              To address these gaps, a Mini AI GRC Framework is proposed, which emphasizes five domains: governance,
              risk, compliance, transparency, and accountability. Each domain is mapped to practical controls and
              existing standards. A real-world example related to AI-driven hiring demonstrates how organizations can
              manage bias, conduct impact assessments, and establish incident response plans to maintain trust and
              compliance.
            </p>
            <p>
              The findings indicate that effective AI governance requires not only adherence to regulations but also a
              continuous commitment to ethical design, collaboration with stakeholders, and adaptive risk management.
              This approach positions responsible AI as both a compliance issue and a business advantage.
            </p>
          </section>

          <section class="article-section">
            <h2>Introduction</h2>
            <p>
              Artificial intelligence (AI) is no longer just a futuristic concept. It is rapidly becoming the
              decision-maker behind hiring processes, loan approvals, medical diagnoses, and even law enforcement
              strategies. While these applications promise efficiency and innovation, they also raise unsettling
              questions. What happens when an algorithm discriminates? Who is accountable when a machine’s decision
              leads to harm? These questions emphasize the growing tension between the speed of adoption of AI and the
              slower pace of governance and regulatory oversight.
            </p>
            <p>
              Traditional frameworks for governance, risk management, and compliance (GRC), such as ISO 27001 or COSO
              ERM, were not designed to manage the unique risks of AI systems. Issues such as algorithmic bias, lack of
              transparency, and evolving regulatory standards stretch conventional compliance models beyond their limits.
              Biased training data can perpetuate discrimination, while opaque “black box” models make it difficult for
              organizations to explain or defend automated decisions. Regulators are beginning to respond, such as the
              NIST AI Risk Management Framework, which provides a structured approach to mapping, measuring, and
              managing AI risks, while the EU Artificial Intelligence Act aims to classify and regulate AI systems by
              risk level.
            </p>
            <p>
              This article looks at how organizations can modify existing GRC models to better manage the challenges
              posed by AI. It discusses the limitations of current approaches, introduces new regulatory ideas such as
              the NIST AI Risk Management Framework and the EU AI Act, and suggests a simple mini-framework tailored to
              govern AI effectively. As businesses continue to adopt smart technologies, creating responsible and
              transparent governance strategies will be crucial, not only to comply with regulations but also to build
              trust and resilience for the future.
            </p>
          </section>

          <section class="article-section">
            <h2>AI-Specific Risks in Governance, Risk, and Compliance</h2>
            <p>
              AI poses a variety of risks that go beyond what traditional GRC frameworks typically cover. Unlike
              standard technologies, AI systems are dynamic, meaning they learn from new data and can adapt to changes,
              sometimes behaving in ways that their creators cannot fully understand. This unpredictability complicates
              oversight and can lead to ethical, legal, and reputational issues.
            </p>
            <p>
              One major concern is algorithmic bias. It is unrealistic to expect that bias can be eliminated from AI
              systems. Instead, companies need to adopt structured methods to identify, track, and correct any unfair
              outcomes. If unchecked, biased algorithms, especially in critical areas like hiring or lending, can
              reinforce existing inequalities and leave organizations open to lawsuits and damage to their reputations.
            </p>
            <p>
              Another significant risk is the lack of transparency and explainability in many advanced AI systems, which
              often operate as “black boxes.” This makes it difficult to audit or justify their decisions, undermining
              compliance efforts and eroding public trust. This is particularly concerning when AI systems make
              life-altering decisions, such as denying medical treatment. Organizations must be able to explain those
              decisions clearly and understandably.
            </p>
            <p>
              Regulatory uncertainty also complicates how businesses adopt AI. Policymakers worldwide are still figuring
              out how to balance the need for economic growth and innovation with the necessity for safety measures.
              This uncertainty can lead to practical issues. For example, organizations using AI for surveillance may
              ignore risks such as disproportionate impacts on marginalized communities. These realities show that
              static compliance checklists are insufficient and that AI demands ongoing monitoring and adaptable risk
              management strategies.
            </p>
            <p>
              Finally, strategic and organizational risks play a crucial role. Board members must prioritize AI
              governance, ensuring it is integrated into the overall enterprise strategy rather than treated as just a
              technical issue. AI should be seen as a long-term societal challenge, not just a short-term compliance
              matter. Taken together, these factors indicate that AI risks are complex and multifaceted, covering areas
              like bias, lack of transparency, regulatory challenges, and governance shortcomings—all of which
              necessitate a reevaluation of traditional GRC frameworks.
            </p>
          </section>

          <section class="article-section">
            <h2>Governance for AI</h2>
            <p>
              Strong governance is crucial for the responsible adoption of artificial intelligence. While traditional
              corporate governance has focused on accountability and oversight, AI presents unique challenges that
              require new frameworks to address its specific risks. Effective AI governance should not be limited to
              technical teams. It needs to involve people, processes, and technology across the organization to ensure
              ethical and transparent use.
            </p>
            <p>
              One effective solution is to establish an AI Governance Board or a similar oversight committee. Board-level
              involvement is essential because AI decisions can significantly impact a company's reputation, financial
              success, and ethical standards. If leadership does not engage in AI governance, organizations may
              implement AI in ways that contradict their stated values. The board does not need to understand the
              technical details of every model but should set clear expectations, demand accountability, and ensure
              alignment with legal and ethical standards.
            </p>
            <p>
              Another important aspect is the use of clear accountability frameworks. Defining roles for each stage of
              the AI lifecycle—data collection, model development, testing, deployment, and monitoring—can prevent
              governance issues caused by ambiguity. Tools like a RACI (Responsible, Accountable, Consulted, Informed)
              matrix help clarify these roles and ensure that compliance officers and technical experts know what is
              expected of them.
            </p>
            <p>
              Transparency and documentation are also vital. Applying frameworks like the NIST AI Risk Management
              Framework to real-world systems requires organizations to document not just technical risks but also
              social impacts such as fairness and privacy. Tools like model cards and datasheets for datasets give
              stakeholders clear information about how models were trained, what data was used, and any limitations.
              This kind of transparency demonstrates a commitment to ethical AI design and helps manage potential
              societal risks before they escalate into regulatory issues.
            </p>
            <p>
              Lastly, effective governance must be collaborative and adaptable. Simply having principles is not enough
              for AI oversight. Governance should aim to build relationships across different parts of the organization
              to account for AI’s unpredictable nature. In this way, AI governance becomes less about strict rules and
              more about fostering cooperation among compliance teams, engineers, executives, and external stakeholders.
            </p>
          </section>

          <section class="article-section">
            <h2>Risk Management Adjustments</h2>
            <p>
              Traditional risk management practices need to adapt to the challenges posed by AI. Frameworks like ISO
              27001 and COSO ERM are strong for managing information security, operational continuity, and financial
              risks, but they do not fully address AI-specific concerns. Risk management strategies must expand to
              include ongoing monitoring, ethical considerations, and the unpredictable behavior of AI systems.
            </p>
            <p>
              Bias and fairness testing are crucial. Instead of trying to eliminate bias, organizations should focus on
              practical ways to mitigate it, acknowledging that data imperfections are inevitable. This includes regular
              fairness audits, using diverse training data, and implementing adversarial testing models that go beyond
              simple compliance checks. Without these measures, companies might meet current regulations but still face
              significant risks if public expectations change.
            </p>
            <p>
              Model drift and continuous monitoring are also key concerns. AI models often perform well when first
              trained, but their performance can degrade as real-world conditions change. Organizations should implement
              ongoing monitoring to catch these shifts early, using checkpoints for retraining, tracking changes, and
              incorporating monitoring throughout the system development lifecycle (SDLC). Explainability dashboards and
              similar tools can help compliance officers and executives oversee AI performance.
            </p>
            <p>
              Incident response and contingency planning must also be updated. Failures related to AI present not only
              operational risks but also systemic challenges. Businesses need crisis plans that account for the
              complexities of algorithmic decision-making. For example, if an autonomous system leads to harmful outcomes,
              companies should have protocols to roll back the system, notify affected users, and comply with regulators.
              Integrating AI-specific response strategies into disaster recovery and business continuity plans helps
              organizations recover without losing public trust.
            </p>
            <p>
              Finally, adapting to changing regulations must be treated as a core part of risk management. Global rules
              are constantly evolving, creating uncertainty for businesses that operate across borders. Organizations
              should monitor legislative changes, maintain flexible frameworks, and recognize that ethical risks add
              complexity that rigid controls alone cannot address.
            </p>
          </section>

          <section class="article-section">
            <h2>Compliance Challenges</h2>
            <p>
              Even with improved governance and risk management, enterprises must confront the shifting landscape of
              compliance. AI introduces novel legal and ethical dilemmas that existing regulatory frameworks were not
              designed to resolve. Regulations such as the GDPR and CCPA mainly focus on data privacy, but neither
              fully addresses how automated decision-making systems should be audited or held accountable. As a result,
              organizations are forced to interpret and adapt older laws to fit the realities of AI, often with
              inconsistent results.
            </p>
            <p>
              Newer initiatives aim to fill these gaps. The EU AI Act introduces a risk-based approach, classifying AI
              systems from minimal to “unacceptable” risk. This pushes enterprises to consider not only whether systems
              function properly, but also whether they fall into categories that trigger stricter obligations such as
              transparency reporting, human oversight, or prohibition. In the U.S., the NIST AI Risk Management
              Framework offers a complementary tool for aligning compliance practices with AI risk assessment.
            </p>
            <p>
              Yet compliance challenges extend beyond legal frameworks. Even well-designed regulations cannot anticipate
              every risk posed by increasingly autonomous AI systems. Compliance must be paired with ethical reflection.
              Organizations should treat AI oversight as a relational practice in which responsibilities are negotiated
              in real time across compliance teams, engineers, and stakeholders. This perspective highlights that
              compliance is dynamic and must evolve as AI systems and their impacts evolve.
            </p>
            <p>
              Industry-specific requirements add another layer of complexity. In regulated domains like healthcare,
              finance, and cybersecurity, AI systems must align with both sector-specific regulations and broader
              ethical expectations. For instance, an AI tool used to analyze medical images must comply with privacy
              rules like HIPAA and also meet standards of clinical accountability and fairness. This “layering” of
              obligations makes it harder for enterprises to stay consistent across jurisdictions and industries.
            </p>
          </section>

          <section class="article-section">
            <h2>Mini AI GRC Framework Proposal</h2>
            <p>
              Traditional GRC methods are useful, but they fall short when dealing with the unique challenges posed by
              AI. Organizations need a simpler, more targeted approach that combines GRC with AI-specific concerns. To
              support that, this article introduces a Mini AI GRC Framework built around five domains: governance, risk,
              compliance, transparency, and accountability.
            </p>

            <div class="blog-table">
              <table>
                <thead>
                  <tr>
                    <th>Domain</th>
                    <th>AI-Specific Risk</th>
                    <th>Suggested Control / Action</th>
                    <th>Reference Standard</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>Governance</td>
                    <td>Lack of oversight and unclear accountability</td>
                    <td>
                      Establish an AI Governance Board and RACI matrices defining roles across the AI lifecycle.
                    </td>
                    <td>NIST AI RMF (Govern function)</td>
                  </tr>
                  <tr>
                    <td>Risk</td>
                    <td>Algorithmic bias and fairness gaps</td>
                    <td>
                      Conduct routine bias testing, fairness audits, and dataset documentation.
                    </td>
                    <td>NIST AI RMF (Map / Measure)</td>
                  </tr>
                  <tr>
                    <td>Compliance</td>
                    <td>Privacy violations and regulatory misalignment</td>
                    <td>
                      Perform Privacy Impact Assessments (PIAs / DPIAs) and monitor evolving regulatory obligations.
                    </td>
                    <td>NIST SP 800-53; GDPR; CCPA</td>
                  </tr>
                  <tr>
                    <td>Transparency</td>
                    <td>Opaque “black box” models</td>
                    <td>
                      Implement explainability tools, model cards, and datasheets for datasets.
                    </td>
                    <td>NIST AI RMF; NIST CSF 2.0</td>
                  </tr>
                  <tr>
                    <td>Accountability</td>
                    <td>Absence of response protocols for AI incidents</td>
                    <td>
                      Develop AI-specific incident response playbooks integrated into DR / BCP strategies.
                    </td>
                    <td>NIST SP 800-61</td>
                  </tr>
                </tbody>
              </table>
            </div>

            <p>
              The framework is grounded in governance research that emphasizes board engagement, structured risk
              controls, fairness considerations, and the integration of ethical principles into compliance practices. By
              aligning with guidance such as the NIST AI RMF and related standards, it remains consistent with
              established best practices while staying flexible enough to adapt to evolving regulatory landscapes.
            </p>
          </section>

          <section class="article-section">
            <h2>Example of Application: AI in Hiring</h2>
            <p>
              To illustrate how this framework can be put into practice, consider a company that uses an AI-driven tool
              to screen job applications.
            </p>
            <p>
              <strong>Governance:</strong> The company sets up an AI Governance Board that includes representatives from
              Human Resources, compliance, and IT. A RACI matrix clarifies roles: HR is responsible for overseeing data
              and use cases, IT is accountable for monitoring the system, and the compliance and legal teams are
              consulted during risk assessments.
            </p>
            <p>
              <strong>Risk Management:</strong> Regular audits are conducted to check for potential bias related to
              gender, race, or age. Findings are documented and reviewed quarterly to maintain fairness and correct
              issues promptly.
            </p>
            <p>
              <strong>Compliance:</strong> Before deploying the AI tool, the organization performs a Privacy Impact
              Assessment to ensure compliance with applicable privacy laws and automated decision-making rules. Local
              employment laws are also reviewed.
            </p>
            <p>
              <strong>Transparency:</strong> The company publishes a model card that explains how the hiring algorithm
              works, what data it was trained on, and its limitations. This supports both candidates and regulators by
              clarifying how automated decisions are made.
            </p>
            <p>
              <strong>Accountability:</strong> An incident response protocol is developed for cases where the tool
              produces discriminatory results. It includes suspending automated decision-making, retraining the model,
              notifying affected applicants, and reporting outcomes to leadership.
            </p>
            <p>
              By using the Mini AI GRC Framework in this context, the organization not only meets regulatory
              requirements but also demonstrates proactive governance and ethical responsibility. This builds trust with
              stakeholders and reduces long-term operational and reputational risk.
            </p>
          </section>

          <section class="article-section">
            <h2>Conclusion</h2>
            <p>
              Artificial intelligence offers exciting possibilities but also presents significant challenges for how
              organizations manage governance, risk, and compliance. Traditional models struggle to keep up with issues
              like algorithmic bias, regulatory uncertainty, and organizational misalignment. Effective AI governance
              requires structured oversight, board-level engagement, and close collaboration between management,
              compliance officers, and technical experts.
            </p>
            <p>
              The Mini AI GRC Framework provides a practical, risk-based approach aligned with standards like the NIST
              AI RMF. By focusing on governance, risk, compliance, transparency, and accountability, it gives
              organizations tools to address AI’s specific challenges while building trust with stakeholders. The hiring
              example shows how these ideas translate into concrete practices such as bias testing and AI-specific
              incident response.
            </p>
            <p>
              Looking ahead, regulations such as the EU AI Act indicate that compliance demands will continue to evolve.
              Organizations must be ready to adjust their frameworks to stay aligned with both legal obligations and
              societal expectations. In that sense, AI governance is not a one-time project but an ongoing process that
              must adapt as technologies and risks change.
            </p>
            <p>
              In summary, AI challenges organizations to rethink governance, risk, and compliance as dynamic and
              collaborative practices. By integrating insights from multiple disciplines and aligning with federal
              standards, organizations can create frameworks that not only meet regulatory requirements but also promote
              ethical responsibility and public trust.
            </p>
          </section>

          <section class="article-section">
            <h2>Annotated Bibliography</h2>
            <p><strong>Coeckelbergh, M. (2024).</strong> From principles to relationships: Redesigning ethics for AI’s alien cognition. <em>AI &amp; Society, 39</em>(2), 345–356.</p>
            <p>
              This source frames AI governance as a flexible, evolving process rather than a fixed set of rules. It
              underscores the importance of adaptability, collaboration, and communication across teams when managing AI
              risks and responsibilities.
            </p>

            <p><strong>Hendrycks, D. (2024).</strong> Introduction to AI safety, ethics, and society. <em>arXiv</em>.</p>
            <p>
              Hendrycks’ work highlights potential long-term risks posed by AI to society. It supports the view that
              businesses must incorporate safety, ethics, and contingency planning into AI risk and compliance
              frameworks.
            </p>

            <p><strong>Institute of Internal Auditors. (2022).</strong> <em>Cybersecurity frameworks: Virtual symposium presentation</em> [PDF].</p>
            <p>
              This resource emphasizes that traditional risk approaches like ISO and COSO are not sufficient for AI
              risks. It argues for enhanced strategies that include continual monitoring, ethical checks, and
              AI-specific controls.
            </p>

            <p><strong>Kulothungan, V., Mohan, P. R., &amp; Gupta, D. (2025).</strong> AI regulation and capitalist growth: Balancing innovation, ethics, and global governance. <em>BigDataSecurity</em>.</p>
            <p>
              The article discusses global regulatory challenges for AI and stresses the need for flexible systems that
              can adapt to evolving standards, such as those introduced by the EU AI Act.
            </p>

            <p><strong>National Institute of Standards and Technology. (2023).</strong> <em>Artificial intelligence risk management framework (AI RMF 1.0)</em>.</p>
            <p>
              NIST’s AI RMF provides foundational guidance for the Mini AI GRC Framework, informing key domains like
              governance, risk, and transparency and supporting practical examples such as AI in hiring.
            </p>

            <p><strong>National Institute of Standards and Technology. (2020).</strong> <em>Security and privacy controls for information systems and organizations (NIST SP 800-53, Rev. 5)</em>.</p>
            <p>
              This document underpins recommended privacy and security practices, including impact assessments and
              alignment with laws such as GDPR and CCPA.
            </p>

            <p><strong>National Institute of Standards and Technology. (2012).</strong> <em>Computer security incident handling guide (NIST SP 800-61, Rev. 2)</em>.</p>
            <p>
              NIST SP 800-61 supports the development of AI-specific incident response plans that account for ethical
              concerns and user impact.
            </p>

            <p><strong>Sayles, J. (2024).</strong> <em>Principles of AI governance and model risk management: Master the techniques for ethical and transparent AI systems</em>. Apress.</p>
            <p>
              Sayles’ work informs ideas around role clarity, lifecycle oversight, and transparency tooling, and
              reinforces the need to embed AI governance into overall organizational strategy.
            </p>

            <p><strong>Sharma, R. (2024).</strong> <em>AI and the boardroom: Insights into governance, strategy, and the responsible adoption of AI</em>. Apress.</p>
            <p>
              Sharma emphasizes board-level engagement in AI strategy and governance, arguing that AI must be treated as
              a core business consideration rather than a side project.
            </p>

            <p><strong>Swaminathan, N., &amp; Danks, D. (2024).</strong> Application of the NIST AI Risk Management Framework to surveillance technology. <em>Journal of AI, Ethics, and Society</em>.</p>
            <p>
              This source illustrates how the NIST AI RMF can be applied to real-world use cases, particularly regarding
              fairness and transparency in surveillance, and argues against rigid checklist-only approaches.
            </p>

            <p><strong>Townson, S. (2023).</strong> Manage AI bias instead of trying to eliminate it. <em>MIT Sloan Management Review, 64</em>(3), 42–47.</p>
            <p>
              Townson focuses on algorithmic bias and fairness testing, offering practical steps for dealing with
              imperfect data rather than promising to eliminate all bias. This perspective is central to AI risk
              management and compliance.
            </p>
          </section>

          <section class="article-section">
            <h2>Artificial Intelligence Disclosure</h2>
            <p>
              In the research and writing process behind this article, AI tools were used to support organization,
              structure, and clarity. AI assisted with outlining sections, suggesting transitions, managing citations,
              and formatting references in APA style, while care was taken to preserve the author’s voice and critical
              thinking. Final responsibility for the analysis, arguments, and conclusions remains with the author.
            </p>
          </section>
            </article>
          </div>
        </article>

      </div>
    </div>
  </section>
</main>


  <footer class="site-footer">
    <div class="container footer-inner">
      <span>&copy; <span id="year"></span> Terrell Grenyion</span>
      <span class="divider">•</span>
      <span>GRC • Risk • Privacy • Security Operations</span>
    </div>
  </footer>

  <script>
  // Footer year
  const yearEl = document.getElementById("year");
  if (yearEl) {
    yearEl.textContent = new Date().getFullYear();
  }

  // Multi-card blog expand / collapse (one button per card)
  (function () {
    const toggles = document.querySelectorAll(".blog-toggle-btn");

    toggles.forEach((btn) => {
      const targetId = btn.getAttribute("data-expand-target");
      const wrapper = document.getElementById(targetId);
      if (!wrapper) return;

      let expanded = false;

      // Start collapsed
      wrapper.style.maxHeight = "0px";
      wrapper.setAttribute("aria-expanded", "false");

      btn.addEventListener("click", () => {
        expanded = !expanded;

        if (expanded) {
          const fullHeight = wrapper.scrollHeight;
          wrapper.classList.add("expanded");
          wrapper.style.maxHeight = fullHeight + "px";
          wrapper.setAttribute("aria-expanded", "true");
          btn.textContent = "Read less";
        } else {
          wrapper.classList.remove("expanded");
          wrapper.style.maxHeight = "0px";
          wrapper.setAttribute("aria-expanded", "false");
          btn.textContent = "Read more";

          // Scroll back to the top of the card on collapse
          const card = btn.closest(".blog-card");
          if (card) {
            const y = card.getBoundingClientRect().top + window.scrollY - 80;
            window.scrollTo({ top: y, behavior: "smooth" });
          }
        }
      });

      // Recalculate height on resize if expanded
      window.addEventListener("resize", () => {
        if (expanded) {
          wrapper.style.maxHeight = wrapper.scrollHeight + "px";
        }
      });
    });
  })();
</script>
  <script>
    (function () {
      const toggle = document.querySelector('.theme-toggle');
      if (!toggle) return;

      toggle.addEventListener('click', function () {
        if (typeof window.__toggleTheme === 'function') {
          window.__toggleTheme();
        }
      });
    })();
  </script>
</body>
</html>








